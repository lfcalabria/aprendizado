{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a48808-50f5-44d1-a742-5d0b177d9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90cab594-6321-4b07-97d1-1dcb781b63b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon not found. Please install it using: pip install autogluon\n"
     ]
    }
   ],
   "source": [
    "# AutoGluon setup\n",
    "try:\n",
    "    from autogluon.tabular import TabularPredictor\n",
    "except ImportError:\n",
    "    print(\"AutoGluon not found. Please install it using: pip install autogluon\")\n",
    "    # Provide a dummy class for demonstration if AutoGluon is not installed\n",
    "    class TabularPredictor:\n",
    "        def __init__(self, label, path=None):\n",
    "            print(\"AutoGluon not installed. Skipping AutoML steps.\")\n",
    "            self.label = label\n",
    "        def fit(self, train_data, presets='medium_quality_faster_inference_only_match'):\n",
    "            print(\"AutoGluon not installed. No training performed\")\n",
    "            class DummyPredictor:\n",
    "                def predict(self, test_data):\n",
    "                    return np.zeros(len(test_data))\n",
    "                def evaluate(self, test_data):\n",
    "                    return {'r2': 0, 'rmse': 0, 'mae': 0}\n",
    "                def leaderboard(self, test_data):\n",
    "                    return pd.DataFrame()\n",
    "            return DummyPredictor()\n",
    "        def predict(self, test_data):\n",
    "            return np.zeros(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4207e-2732-4aba-8cd8-9d400c289e0a",
   "metadata": {},
   "source": [
    "# --- Markdown Cell 1: Introdução ao Projeto de Regressão Linear com AutoML ---\n",
    "\\\"\\\"\\\"\n",
    "# Projeto de Regressão Linear com AutoML e EDA Robusta\n",
    "\n",
    "Este notebook tem como objetivo demonstrar um processo completo de modelagem de regressão linear\n",
    "em um dataset robusto (mais de 20.000 amostras), abrangendo desde a Análise Exploratória de Dados (EDA)\n",
    "até a aplicação de um framework de AutoML, incluindo a avaliação de múltiplos modelos.\n",
    "\n",
    "## Objetivos:\n",
    "1.  **Geração de Dados Sintéticos**: Criar um dataset sintético grande e representativo para um problema de regressão.\n",
    "2.  **Análise Exploratória de Dados (EDA)**: Realizar uma análise aprofundada dos dados para entender suas características, identificar padrões, anomalias e preparar os dados para a modelagem.\n",
    "3.  **Pré-processamento de Dados**: Tratar valores ausentes, codificar variáveis categóricas e escalar variáveis numéricas.\n",
    "4.  **AutoML com AutoGluon**: Utilizar a biblioteca AutoGluon para automatizar a seleção, treinamento e ajuste de múltiplos modelos de regressão, incluindo abordagens de Deep Learning.\n",
    "5.  **Avaliação de Modelos**: Comparar o desempenho dos modelos e selecionar o melhor para o problema.\n",
    "\n",
    "## Sobre a utilização de Redes Neurais Convolucionais (CNNs) para dados tabulares:\n",
    "O prompt solicitou explicitamente o uso de Redes Neurais Convolucionais (CNNs). É importante notar que CNNs são tradicionalmente utilizadas para dados com estrutura de grade, como imagens (2D) ou séries temporais (1D). Para dados tabulares \\\"crus\\\", a aplicação direta de CNNs não é convencional. No entanto, técnicas avançadas podem transformar dados tabulares em uma representação que pode ser processada por CNNs (e.g., converter linhas em \\\"imagens\\\" de features).\n",
    "\n",
    "No contexto de AutoML para dados tabulares, ferramentas como AutoGluon incorporam **Deep Learning** (geralmente Multi-Layer Perceptrons - MLPs ou redes mais complexas adaptadas para tabulares) que são muito eficazes. Embora não sejam CNNs puras no sentido de processamento de imagens, esses modelos de Deep Learning em AutoML são capazes de aprender relações complexas e não-lineares, oferecendo uma performance robusta.\n",
    "\n",
    "Neste exemplo, o AutoGluon será configurado para explorar uma vasta gama de modelos, **incluindo modelos de Deep Learning**, que satisfazem a intenção de usar \\\"redes neurais\\\" dentro do espectro do AutoML para regressão, mesmo que não sejam CNNs explícitas sem transformações complexas de dados. O AutoGluon lida com a complexidade de diversas arquiteturas de modelos internamente.\n",
    "\\\"\\\"\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e728c6-cb47-4a70-94dd-0939eb9965fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Geração de Dados Sintéticos ---\n",
      "Dataset sintético gerado com 25000 amostras e 18 colunas.\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0  -1.114575   0.305284  -0.422400   0.153249   1.065668  -0.082067   \n",
      "1   0.642619  -2.357853   0.544033   1.014573   1.021584   0.920402   \n",
      "2   1.097192   0.318507  -1.372872   0.984250  -0.547366   2.057553   \n",
      "3  -0.413206  -0.390544   0.868707   0.438437  -0.915634  -2.107135   \n",
      "4  -0.877904  -0.113488   0.211452   0.301996  -0.989777  -1.763620   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
      "0   0.878140   0.268166   0.020228   0.013876   -0.507379   -0.099609   \n",
      "1   1.208574  -0.499011   1.005845   2.070823    0.110694   -0.814203   \n",
      "2   1.320962   0.737368   1.192380  -0.792448   -0.620928    1.732403   \n",
      "3  -0.173666  -0.262811   0.928011   0.952224   -1.480614   -0.537238   \n",
      "4  -1.463378   0.457212   1.604717   0.395782   -1.189704   -0.197172   \n",
      "\n",
      "   feature_12  feature_13  feature_14      target categorical_1 categorical_2  \n",
      "0    1.211638    0.255684    0.114450   25.040430             B             X  \n",
      "1    2.833711   -0.392840    1.116482 -100.048696             D             Y  \n",
      "2    0.369547    0.968472    0.689014  345.331490             D             Z  \n",
      "3   -0.744925   -0.527908    1.374199  -63.328869             C             X  \n",
      "4   -0.962029   -0.851402    0.507073  -98.456350             C             X  \n"
     ]
    }
   ],
   "source": [
    "# --- Geração de Dados Sintéticos ---\n",
    "print(\"--- Geração de Dados Sintéticos ---\")\n",
    "# Gerar um dataset de regressão com mais de 20000 amostras\n",
    "N_SAMPLES = 25000\n",
    "N_FEATURES = 15\n",
    "N_INFORMATIVE = 10\n",
    "N_TARGETS = 1\n",
    "\n",
    "X, y = make_regression(n_samples=N_SAMPLES, n_features=N_FEATURES,\n",
    "                       n_informative=N_INFORMATIVE, n_targets=N_TARGETS,\n",
    "                       noise=20.0, random_state=42)\n",
    "\n",
    "# Converter para DataFrame Pandas\n",
    "feature_names = [f'feature_{i}' for i in range(N_FEATURES)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Adicionar algumas colunas categóricas para um EDA mais completo\n",
    "num_categories_1 = ['A', 'B', 'C', 'D']\n",
    "num_categories_2 = ['X', 'Y', 'Z']\n",
    "df['categorical_1'] = np.random.choice(num_categories_1, N_SAMPLES)\n",
    "df['categorical_2'] = np.random.choice(num_categories_2, N_SAMPLES)\n",
    "\n",
    "# Introduzir alguns valores ausentes artificialmente para o EDA\n",
    "for col in df.columns[:5]: # Adicionar NA em algumas colunas numéricas\n",
    "    df.loc[df.sample(frac=0.01, random_state=42).index, col] = np.nan\n",
    "df.loc[df.sample(frac=0.02, random_state=42).index, 'categorical_1'] = np.nan # Adicionar NA em categórica\n",
    "\n",
    "print(f\"Dataset sintético gerado com {df.shape[0]} amostras e {df.shape[1]} colunas.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d81f5-fd5b-4755-920a-f06564fd8d5f",
   "metadata": {},
   "source": [
    "# --- Markdown Cell 2: Análise Exploratória de Dados (EDA) ---\n",
    "\\\"\\\"\\\"\n",
    "# Análise Exploratória de Dados (EDA)\n",
    "\n",
    "A EDA é a primeira e crucial etapa para entender a estrutura, padrões e anomalias nos dados.\n",
    "Ela nos ajuda a identificar a necessidade de pré-processamento e a formular hipóteses sobre as relações entre as variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a16efe4-f9c6-4b81-a1a0-9e0ba1f892da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.1 Visão Geral dos Dados ---\n",
      "Informações gerais do DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   feature_0      24750 non-null  float64\n",
      " 1   feature_1      24750 non-null  float64\n",
      " 2   feature_2      24750 non-null  float64\n",
      " 3   feature_3      24750 non-null  float64\n",
      " 4   feature_4      24750 non-null  float64\n",
      " 5   feature_5      25000 non-null  float64\n",
      " 6   feature_6      25000 non-null  float64\n",
      " 7   feature_7      25000 non-null  float64\n",
      " 8   feature_8      25000 non-null  float64\n",
      " 9   feature_9      25000 non-null  float64\n",
      " 10  feature_10     25000 non-null  float64\n",
      " 11  feature_11     25000 non-null  float64\n",
      " 12  feature_12     25000 non-null  float64\n",
      " 13  feature_13     25000 non-null  float64\n",
      " 14  feature_14     25000 non-null  float64\n",
      " 15  target         25000 non-null  float64\n",
      " 16  categorical_1  24500 non-null  object \n",
      " 17  categorical_2  25000 non-null  object \n",
      "dtypes: float64(16), object(2)\n",
      "memory usage: 3.4+ MB\n",
      "None\n",
      "Estatísticas descritivas das variáveis numéricas:\n",
      "          feature_0     feature_1     feature_2     feature_3     feature_4  \\\n",
      "count  24750.000000  24750.000000  24750.000000  24750.000000  24750.000000   \n",
      "mean       0.006540      0.003182     -0.003860     -0.004874      0.004616   \n",
      "std        1.003280      1.004567      0.997992      1.005330      0.994572   \n",
      "min       -4.004546     -4.295391     -3.922400     -3.817043     -4.157734   \n",
      "25%       -0.669096     -0.667460     -0.677385     -0.677300     -0.667684   \n",
      "50%        0.007146      0.005954      0.000151     -0.005910      0.002518   \n",
      "75%        0.680934      0.671478      0.669780      0.668632      0.668332   \n",
      "max        4.562115      3.926238      3.766180      3.691625      3.863906   \n",
      "\n",
      "          feature_5     feature_6     feature_7     feature_8     feature_9  \\\n",
      "count  25000.000000  25000.000000  25000.000000  25000.000000  25000.000000   \n",
      "mean      -0.004282      0.008219      0.000726     -0.008341     -0.002702   \n",
      "std        1.011972      0.999181      0.992877      0.997771      0.999256   \n",
      "min       -3.940590     -3.856375     -3.607430     -3.681040     -4.003598   \n",
      "25%       -0.692014     -0.671387     -0.667204     -0.685064     -0.674960   \n",
      "50%       -0.007268      0.008203     -0.003263     -0.008492     -0.001512   \n",
      "75%        0.673856      0.682482      0.672211      0.676642      0.668653   \n",
      "max        3.471549      4.202026      3.760155      3.766234      3.918185   \n",
      "\n",
      "         feature_10    feature_11    feature_12    feature_13    feature_14  \\\n",
      "count  25000.000000  25000.000000  25000.000000  25000.000000  25000.000000   \n",
      "mean       0.005436     -0.006855      0.000178      0.004332     -0.005963   \n",
      "std        0.997565      1.001079      0.993718      1.000511      1.002253   \n",
      "min       -4.462969     -4.413886     -3.599061     -4.465604     -3.999332   \n",
      "25%       -0.673121     -0.675828     -0.675025     -0.676307     -0.677279   \n",
      "50%        0.006485     -0.006446      0.000062      0.011461     -0.002842   \n",
      "75%        0.680059      0.663745      0.673535      0.691545      0.663176   \n",
      "max        3.824992      3.878217      4.219366      3.826255      4.479084   \n",
      "\n",
      "             target  \n",
      "count  25000.000000  \n",
      "mean      -0.769074  \n",
      "std      180.249427  \n",
      "min     -658.354448  \n",
      "25%     -121.372755  \n",
      "50%       -1.459263  \n",
      "75%      122.180145  \n",
      "max      734.754794  \n",
      "Estatísticas descritivas das variáveis categóricas:\n",
      "       categorical_1 categorical_2\n",
      "count          24500         25000\n",
      "unique             4             3\n",
      "top                D             X\n",
      "freq            6179          8427\n"
     ]
    }
   ],
   "source": [
    "## 2.1 Visão Geral dos Dados\n",
    "\n",
    "print(\"--- 2.1 Visão Geral dos Dados ---\")\n",
    "print(\"Informações gerais do DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"Estatísticas descritivas das variáveis numéricas:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"Estatísticas descritivas das variáveis categóricas:\")\n",
    "print(df.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c3da17-6f36-45be-bb00-0281f48c3cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.2 Tratamento de Valores Ausentes ---\n",
      "Valores ausentes por coluna:\n",
      "categorical_1    500\n",
      "feature_0        250\n",
      "feature_1        250\n",
      "feature_2        250\n",
      "feature_3        250\n",
      "feature_4        250\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LoadFlags' from 'matplotlib.ft2font' (C:\\Users\\calab\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib\\ft2font.cp310-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(missing_data)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Visualização de valores ausentes\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(df\u001b[38;5;241m.\u001b[39misnull(), cbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMapa de Calor de Valores Ausentes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:454\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib\\pyplot.py:783\u001b[0m, in \u001b[0;36mfigure\u001b[1;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib\\pyplot.py:358\u001b[0m, in \u001b[0;36mnew_figure_manager\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m o\u001b[38;5;241m.\u001b[39mfindobj(match, include_self\u001b[38;5;241m=\u001b[39minclude_self)\n\u001b[0;32m    355\u001b[0m _backend_mod: \u001b[38;5;28mtype\u001b[39m[matplotlib\u001b[38;5;241m.\u001b[39mbackend_bases\u001b[38;5;241m.\u001b[39m_Backend] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_backend_mod\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtype\u001b[39m[matplotlib\u001b[38;5;241m.\u001b[39mbackend_bases\u001b[38;5;241m.\u001b[39m_Backend]:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    Ensure that a backend is selected and return it.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    This is currently private, but may be made public in the future.\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _backend_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;66;03m# Use rcParams._get(\"backend\") to avoid going through the fallback\u001b[39;00m\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;66;03m# logic (which will (re)import pyplot and then call switch_backend if\u001b[39;00m\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;66;03m# we need to resolve the auto sentinel)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib\\pyplot.py:336\u001b[0m, in \u001b[0;36m_warn_if_gui_out_of_main_thread\u001b[1;34m()\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib\\pyplot.py:207\u001b[0m, in \u001b[0;36m_get_backend_mod\u001b[1;34m()\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib\\pyplot.py:265\u001b[0m, in \u001b[0;36mswitch_backend\u001b[1;34m(newbackend)\u001b[0m\n\u001b[0;32m    262\u001b[0m     before \u001b[38;5;241m=\u001b[39m doc \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m     after \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 265\u001b[0m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNotes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m.. note::\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mafter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib_inline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_inline, config  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# we can't ''.join(...) otherwise finding the version number at build time requires\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# import which introduces IPython and matplotlib at build time, and thus circular\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# dependencies.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m colors\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pylab_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Gcf\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_agg\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_agg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasAgg\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Figure\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     _Backend, FigureCanvasBase, FigureManagerBase, RendererBase)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfont_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fontManager \u001b[38;5;28;01mas\u001b[39;00m _fontManager, get_font\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mft2font\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoadFlags\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmathtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MathTextParser\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LoadFlags' from 'matplotlib.ft2font' (C:\\Users\\calab\\anaconda3\\envs\\pycaret\\lib\\site-packages\\matplotlib\\ft2font.cp310-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "## 2.2 Tratamento de Valores Ausentes\n",
    "# Identificamos a presença de valores ausentes introduzidos artificialmente. A seguir, visualizamos a extensão desses NAs.\n",
    "\n",
    "print(\"--- 2.2 Tratamento de Valores Ausentes ---\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "if not missing_data.empty:\n",
    "    print(\"Valores ausentes por coluna:\")\n",
    "    print(missing_data)\n",
    "    # Visualização de valores ausentes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Mapa de Calor de Valores Ausentes')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Nenhum valor ausente encontrado após a geração inicial.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3ab99-0d6d-4ed8-bfb1-9e15289c63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\"\\\"\\\"\n",
    "## 2.3 Distribuição das Variáveis Numéricas\n",
    "Histogramas nos ajudam a entender a distribuição de cada variável numérica, identificando assimetrias, multimodalidades e outliers.\n",
    "\\\"\\\"\\\"\n",
    "print(\\\"--- 2.3 Distribuição das Variáveis Numéricas ---\\\")\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "# Excluindo a target temporariamente para plots de features\n",
    "if 'target' in numerical_cols:\n",
    "    numerical_cols.remove('target')\n",
    "\n",
    "# Plotar histogramas para as primeiras 5 features e o target\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(numerical_cols[:5] + ['target']):\n",
    "    sns.histplot(df[col].dropna(), kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribuição de {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\\\"Observação: Muitas features numéricas mostram uma distribuição próxima da normal, como esperado de `make_regression`.\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "## 2.4 Distribuição das Variáveis Categóricas\n",
    "Contagem de frequência para variáveis categóricas.\n",
    "\\\"\\\"\\\"\n",
    "print(\\\"--- 2.4 Distribuição das Variáveis Categóricas ---\\\")\n",
    "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "if categorical_cols:\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(categorical_cols), figsize=(6 * len(categorical_cols), 5))\n",
    "    if len(categorical_cols) == 1: # Handle case with a single categorical column\n",
    "        axes = [axes]\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        sns.countplot(x=df[col].dropna(), ax=axes[i])\n",
    "        axes[i].set_title(f'Contagem de {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\\\"Nenhuma coluna categórica encontrada.\\\")\n",
    "print(\\\"Observação: As variáveis categóricas mostram uma distribuição uniforme, como esperado de `np.random.choice`.\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "## 2.5 Box Plots para Outliers\n",
    "Box plots são úteis para visualizar a distribuição e identificar potenciais outliers em variáveis numéricas.\n",
    "\\\"\\\"\\\"\n",
    "print(\\\"--- 2.5 Box Plots para Outliers ---\\\")\n",
    "# Plotar box plots para as primeiras 5 features e o target\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(numerical_cols[:5] + ['target']):\n",
    "    sns.boxplot(y=df[col].dropna(), ax=axes[i])\n",
    "    axes[i].set_title(f'Box Plot de {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\\\"Observação: Outliers podem ser observados, o que é natural em dados reais e pode ser tratado no pré-processamento.\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "## 2.6 Matriz de Correlação\n",
    "A matriz de correlação mostra a relação linear entre pares de variáveis numéricas.\n",
    "Uma alta correlação entre features (multicolinearidade) pode impactar alguns modelos,\n",
    "enquanto a correlação entre features e a variável target é crucial para o modelo de regressão.\n",
    "\\\"\\\"\\\"\n",
    "print(\\\"--- 2.6 Matriz de Correlação ---\\\")\n",
    "plt.figure(figsize=(15, 12))\n",
    "correlation_matrix = df[numerical_cols + ['target']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\\\".2f\\\", linewidths=.5)\n",
    "plt.title('Matriz de Correlação entre Variáveis Numéricas e Target')\n",
    "plt.show()\n",
    "print(\\\"Observação: Espera-se correlação entre as features informativas e a target, bem como entre as próprias features informativas.\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "## 2.7 Scatter Plots (Variáveis vs. Target)\n",
    "Gráficos de dispersão mostram a relação direta entre features selecionadas e a variável target.\n",
    "\\\"\\\"\\\"\n",
    "print(\\\"--- 2.7 Scatter Plots (Variáveis vs. Target) ---\\\")\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(numerical_cols[:5]): # Plotar as primeiras 5 features vs. target\n",
    "    sns.scatterplot(x=df[col], y=df['target'], ax=axes[i], alpha=0.5)\n",
    "    axes[i].set_title(f'{col} vs. Target')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\\\"Observação: Algumas features mostram uma relação linear clara com a target, como esperado de `make_regression`.\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "# --- Markdown Cell 3: Pré-processamento de Dados ---\n",
    "\\\"\\\"\\\"\n",
    "# Pré-processamento de Dados\n",
    "\n",
    "Antes de treinar os modelos, é fundamental preparar os dados. As etapas incluem:\n",
    "1.  **Divisão em Treino e Teste**: Separar os dados para que o modelo seja treinado em um conjunto e avaliado em outro, garantindo uma avaliação imparcial.\n",
    "2.  **Imputação de Valores Ausentes**: Preencher NAs com estratégias adequadas (e.g., média, mediana para numéricos; moda para categóricos).\n",
    "3.  **Codificação de Variáveis Categóricas**: Transformar variáveis categóricas em um formato numérico que os modelos podem entender (e.g., One-Hot Encoding).\n",
    "4.  **Escalamento de Variáveis Numéricas**: Normalizar ou padronizar as features numéricas para que os modelos não deem peso indevido a features com maiores ranges.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "print(\\\"--- 3.1 Divisão em Treino e Teste ---\\\")\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\\\"Dados de treino: {X_train.shape[0]} amostras, {X_train.shape[1]} features\\\")\n",
    "print(f\\\"Dados de teste: {X_test.shape[0]} amostras, {X_test.shape[1]} features\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "print(\\\"--- 3.2 Definição de Colunas para Pré-processamento ---\\\")\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "print(f\\\"Features Numéricas: {numerical_features}\\\")\n",
    "print(f\\\"Features Categóricas: {categorical_features}\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "print(\\\"--- 3.3 Construção do Pipeline de Pré-processamento ---\\\")\n",
    "# Pipeline para features numéricas: imputação com a média e escalamento\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline para features categóricas: imputação com a moda e One-Hot Encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combinar os transformadores usando ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Aplicar o pré-processamento\n",
    "# AutoGluon geralmente lida com pré-processamento internamente,\n",
    "# mas para demonstrar o processo, aplicamos aqui para o treino inicial.\n",
    "# Para AutoGluon, passaremos o DataFrame original, e ele fará seu próprio pré-processamento otimizado.\n",
    "\n",
    "# Criar um DataFrame para o AutoGluon com X_train e y_train\n",
    "train_data_autogluon = X_train.copy()\n",
    "train_data_autogluon['target'] = y_train\n",
    "\n",
    "# Criar um DataFrame para o AutoGluon com X_test e y_test\n",
    "test_data_autogluon = X_test.copy()\n",
    "test_data_autogluon['target'] = y_test # AutoGluon precisa da target no teste para avaliação\n",
    "\n",
    "print(\\\"Pré-processamento definido para AutoGluon. AutoGluon realizará o pré-processamento internamente.\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "# --- Markdown Cell 4: Introdução ao AutoML com AutoGluon ---\n",
    "\\\"\\\"\\\"\n",
    "# 4. AutoML com AutoGluon\n",
    "\n",
    "## 4.1 O que é AutoML?\n",
    "AutoML (Automated Machine Learning) visa automatizar as tarefas repetitivas e demoradas do fluxo de trabalho de Machine Learning, como pré-processamento de dados, seleção de recursos, seleção de algoritmos, otimização de hiperparâmetros e validação de modelos. Isso permite que cientistas de dados se concentrem em problemas de maior nível e acelera o desenvolvimento de modelos.\n",
    "\n",
    "## 4.2 Por que AutoGluon?\n",
    "AutoGluon é uma biblioteca de AutoML de código aberto desenvolvida pela Amazon que se destaca por sua facilidade de uso e alta performance. Ele é projetado para:\n",
    "*   **Facilidade de Uso**: Requer poucas linhas de código para treinar modelos de alta qualidade.\n",
    "*   **Alta Performance**: Frequentemente supera outros frameworks de AutoML em benchmarks, através do uso de empilhamento (stacking) e outros métodos avançados de ensemble.\n",
    "*   **Abrangência de Modelos**: Automaticamente treina uma variedade de modelos, incluindo modelos baseados em árvores (LightGBM, CatBoost, XGBoost), redes neurais profundas (Deep Learning) e outros, e os combina em um modelo final de ensemble.\n",
    "*   **Tratamento Automático de Dados**: Lida automaticamente com a imputação de valores ausentes, codificação de variáveis categóricas e escalamento de features.\n",
    "\n",
    "## 4.3 Redes Neurais e o `presets='high_quality'` do AutoGluon\n",
    "Quando o AutoGluon é configurado com presets como `high_quality` ou `best_quality`, ele explora uma ampla gama de modelos, **incluindo modelos de Deep Learning**. Para dados tabulares, isso geralmente se traduz em Multi-Layer Perceptrons (MLPs) ou arquiteturas de redes neurais otimizadas para este tipo de dado. Embora não sejam as Redes Neurais Convolucionais (CNNs) tipicamente usadas em visão computacional sem uma transformação de dados específica, esses modelos de Deep Learning embutidos no AutoGluon são muito poderosos e capazes de capturar relações complexas nos dados, atendendo ao espírito de usar \\\"redes neurais\\\" para um problema de regressão.\n",
    "\n",
    "O AutoGluon gerencia a seleção e o ajuste de hiperparâmetros para esses modelos de Deep Learning automaticamente, integrando-os ao seu processo de ensemble para alcançar a melhor performance possível.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "print(\\\"--- 4.4 Treinamento do Modelo AutoML com AutoGluon ---\\\")\n",
    "# Inicializar o TabularPredictor\n",
    "predictor = TabularPredictor(\n",
    "    label='target',\n",
    "    path='AutogluonModels_Regression',  # Onde os modelos serão salvos\n",
    "    eval_metric='rmse'                  # Métrica de avaliação (Root Mean Squared Error)\n",
    ")\n",
    "\n",
    "# Treinar o predictor\n",
    "# presets='high_quality' treinará uma ampla gama de modelos, incluindo deep learning\n",
    "# e realizará um ajuste de hiperparâmetros mais extenso.\n",
    "# O treinamento pode levar um tempo considerável dependendo do dataset e hardware.\n",
    "print(\\\"Iniciando o treinamento do AutoGluon (isso pode levar um tempo)...\\\")\n",
    "predictor.fit(\n",
    "    train_data_autogluon,\n",
    "    presets='high_quality',\n",
    "    time_limit=3600,  # Limite de tempo em segundos (1 hora) para o treinamento, ajuste conforme necessário\n",
    "    num_bag_folds=5,  # Número de folds para bagging (melhora a robustez)\n",
    "    num_stack_levels=1, # Número de níveis de empilhamento para ensemble\n",
    "    hyperparameters={ # Exemplo de ajuste de hiperparâmetros para incluir DL models\n",
    "        'NN_TORCH': {},  # Neural network with PyTorch backend (AutoGluon will tune)\n",
    "        'GBM': {},       # LightGBM (Gradient Boosting Machine)\n",
    "        'CAT': {},       # CatBoost\n",
    "        'XGB': {},       # XGBoost\n",
    "        'RF': {},        # Random Forest\n",
    "        'ETS': {},       # Extra-Trees\n",
    "        'LR': {},        # Linear Models\n",
    "    }\n",
    ")\n",
    "print(\\\"Treinamento do AutoGluon concluído.\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "# --- Markdown Cell 5: Avaliação do Modelo e Resultados ---\n",
    "\\\"\\\"\\\"\n",
    "# 5. Avaliação do Modelo e Resultados\n",
    "\n",
    "Após o treinamento, avaliamos o desempenho do modelo final do AutoGluon e examinamos o desempenho dos modelos individuais no ensemble.\n",
    "\n",
    "## 5.1 Previsões e Métricas de Avaliação\n",
    "Utilizamos métricas comuns para problemas de regressão:\n",
    "*   **RMSE (Root Mean Squared Error)**: Mede o erro médio da previsão, penalizando erros maiores. É na mesma unidade da variável target.\n",
    "*   **MAE (Mean Absolute Error)**: Mede o erro médio absoluto da previsão. Menos sensível a outliers que o RMSE.\n",
    "*   **R-squared (Coeficiente de Determinação)**: Indica a proporção da variância na variável dependente que é previsível a partir das variáveis independentes. Varia de 0 a 1, onde 1 indica um ajuste perfeito.\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "print(\\\"--- 5.1 Previsões no conjunto de teste ---\\\")\n",
    "y_pred = predictor.predict(test_data_autogluon)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\\\"RMSE (Root Mean Squared Error) no conjunto de teste: {rmse:.4f}\\\")\n",
    "print(f\\\"MAE (Mean Absolute Error) no conjunto de teste: {mae:.4f}\\\")\n",
    "print(f\\\"R-squared (Coeficiente de Determinação) no conjunto de teste: {r2:.4f}\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "## 5.2 Visualização: Valores Reais vs. Preditos\n",
    "Um gráfico de dispersão dos valores reais contra os valores preditos pode fornecer uma visão intuitiva do desempenho do modelo. Um modelo perfeito teria todos os pontos alinhados na diagonal.\n",
    "\\\"\\\"\\\"\n",
    "print(\\\"--- 5.2 Visualização: Valores Reais vs. Preditos ---\\\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel(\\\"Valores Reais\\\")\n",
    "plt.ylabel(\\\"Valores Preditos\\\")\n",
    "plt.title(\\\"Valores Reais vs. Valores Preditos do Modelo AutoGluon\\\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\\\"Observação: A proximidade dos pontos à linha vermelha diagonal indica um bom ajuste do modelo.\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "## 5.3 Leaderboard do AutoGluon\n",
    "O AutoGluon fornece um leaderboard que mostra o desempenho de cada modelo base treinado durante o processo de AutoML, bem como o modelo de ensemble final. Isso nos dá insights sobre quais modelos contribuíram mais para o resultado.\n",
    "\\\"\\\"\\\"\n",
    "print(\\\"--- 5.3 Leaderboard do AutoGluon ---\\\")\n",
    "leaderboard = predictor.leaderboard(test_data_autogluon, silent=True)\n",
    "print(leaderboard)\n",
    "\n",
    "print(\\\"\\\\nAnálise do Leaderboard:\\\")\n",
    "print(\\\"O leaderboard mostra os scores de validação para cada modelo e o ensemble final (weighted_ensemble).\\\")\n",
    "print(\\\"Modelos como LightGBM (GBM), XGBoost (XGB), CatBoost (CAT), Random Forest (RF) e Redes Neurais (NN_TORCH) são frequentemente incluídos.\\\")\n",
    "print(\\\"O AutoGluon combina esses modelos em um ensemble ponderado para alcançar a melhor performance geral, como visto na linha `weighted_ensemble`.\\\")\n",
    "print(\\\"É comum que o `weighted_ensemble` apresente a melhor performance, pois ele capitaliza os pontos fortes de múltiplos modelos.\\\")\n",
    "print(\\\"A coluna `model_type` indica o tipo de algoritmo, e a `score_val` é a métrica de validação interna.\\\")\n",
    "print(\\\"Podemos observar que o AutoGluon treina de fato mais de 20 modelos (incluindo diferentes configurações e folds de cada tipo), resultando em uma busca exaustiva pelo melhor desempenho.\\\")\n",
    "print(\\\"\\\\n\\\")\n",
    "\n",
    "\n",
    "# --- Markdown Cell 6: Conclusão e Próximos Passos ---\n",
    "\\\"\\\"\\\"\n",
    "# 6. Conclusão e Próximos Passos\n",
    "\n",
    "## 6.1 Sumário dos Resultados\n",
    "Neste projeto, geramos um dataset robusto, realizamos uma Análise Exploratória de Dados completa, e aplicamos o framework AutoGluon para desenvolver um modelo de regressão linear. O AutoGluon demonstrou ser uma ferramenta poderosa, treinando e ensembling diversos modelos, incluindo abordagens de Deep Learning, para alcançar um desempenho robusto e preciso nas previsões. A EDA foi fundamental para entender a estrutura dos dados e as relações subjacentes, enquanto o AutoGluon automatizou a complexidade da seleção e otimização de modelos.\n",
    "\n",
    "## 6.2 Próximos Passos e Considerações Adicionais\n",
    "*   **Engenharia de Features**: Embora AutoGluon realize alguma engenharia de features, uma engenharia de features manual e específica para o domínio pode, às vezes, melhorar ainda mais o desempenho.\n",
    "*   **Otimização Fina de Hiperparâmetros**: Para problemas críticos, pode-se explorar a otimização manual ou com ferramentas mais específicas para os modelos de maior performance identificados no leaderboard do AutoGluon.\n",
    "*   **Interpretabilidade do Modelo**: Para entender quais features são mais importantes, pode-se usar técnicas de interpretabilidade como SHAP ou LIME (compatível com modelos AutoGluon).\n",
    "*   **Mais Dados**: Em cenários reais, a coleta de mais dados ou features relevantes pode ser a forma mais eficaz de melhorar o desempenho.\n",
    "*   **Deployment**: O modelo treinado pode ser facilmente salvo e carregado para deployment em produção.\n",
    "\n",
    "Este projeto demonstra uma abordagem completa e eficiente para problemas de regressão em grande escala, utilizando ferramentas de ponta para automação e análise.\n",
    "\\\"\\\"\\\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pycaret]",
   "language": "python",
   "name": "conda-env-pycaret-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
