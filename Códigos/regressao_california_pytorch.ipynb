{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a6721f",
   "metadata": {},
   "source": [
    "# Regressão completa (California Housing) — EDA, 20+ modelos, AutoML (PyTorch)\n",
    "\n",
    "**Resumo:** Notebook que baixa a base *California Housing* (20.640 amostras) usando `sklearn.datasets`, faz EDA (com `ydata-profiling`), pré-processamento, treina e compara 20+ modelos (sklearn, XGBoost, LightGBM, CatBoost, MLP PyTorch, 1D-CNN PyTorch), usa AutoML (FLAML), e apresenta análise dos resultados com células explicativas.\n",
    "\n",
    "**Observação:** execute as células em ordem. A primeira célula instala dependências necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300af101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala dependências (execute apenas se necessário)\n",
    "# Alguns pacotes são grandes; comente os que você não quiser instalar.\n",
    "#!pip install -q ydata-profiling==4.1.1 pandas matplotlib seaborn missingno scikit-learn xgboost lightgbm catboost flaml optuna torch torchvision\n",
    "\n",
    "# Aviso: instalação do PyTorch pode variar conforme CUDA; aqui instalamos a versão CPU via pip padrão.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a57d35",
   "metadata": {},
   "source": [
    "## 1) Imports e carregamento da base\n",
    "\n",
    "Nesta seção importamos bibliotecas e carregamos a base `fetch_california_housing` do scikit-learn (20.640 amostras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b621496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (20640, 9)\n"
     ]
    }
   ],
   "source": [
    "# Imports básicos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Carrega o dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.copy()\n",
    "display(df.head())\n",
    "print('Shape:', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848deee6",
   "metadata": {},
   "source": [
    "## 2) EDA aprofundado\n",
    "\n",
    "Usaremos `ydata-profiling` para gerar um relatório exploratório e também faremos plots customizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed114dbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numba' has no attribute 'generated_jit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Gera relatório rápido com ydata-profiling (salva em HTML)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m      3\u001b[0m profile \u001b[38;5;241m=\u001b[39m ProfileReport(df, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalifornia Housing - EDA\u001b[39m\u001b[38;5;124m'\u001b[39m, explorative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m profile\u001b[38;5;241m.\u001b[39mto_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalifornia_eda_report.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\ydata_profiling\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Main module of ydata-profiling.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m.. include:: ../../README.md\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompare_reports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compare\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontroller\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pandas_decorator\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile_report\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\ydata_profiling\\compare_reports.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Correlation, Settings\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malerts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Alert\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile_report\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_should_wrap\u001b[39m(v1: Any, v2: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v1, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\ydata_profiling\\profile_report.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtypeguard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m typechecked\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisionsTypeset\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config, Settings, SparkSettings\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpectations_report\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationsReport\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\visions\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Core functionality\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types, typesets, utils\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeclarative\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_type\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     cast_to_detected,\n\u001b[0;32m      8\u001b[0m     cast_to_inferred,\n\u001b[0;32m      9\u001b[0m     detect_type,\n\u001b[0;32m     10\u001b[0m     infer_type,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\visions\\backends\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pandas_version\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pandas_version[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\visions\\backends\\pandas\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraversal\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\visions\\backends\\pandas\\types\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolean\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomplex\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcount\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdate\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\visions\\backends\\pandas\\types\\complex.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m series_not_empty, series_not_sparse\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfloat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m string_is_float\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallelization_engines\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pandas_apply\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomplex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Complex\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m String\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\visions\\backends\\shared\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nan_handling, parallelization_engines, utilities\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pycaret\\lib\\site-packages\\visions\\backends\\shared\\nan_handling.py:34\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# TODO: There are optimizations here, just have to define precisely the desired missing ruleset in the\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# generated jit\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_numba:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;129m@nb\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerated_jit\u001b[49m(nopython\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_missing\u001b[39m(x):\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m        Return True if the value is missing, False otherwise.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, nb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mFloat):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numba' has no attribute 'generated_jit'"
     ]
    }
   ],
   "source": [
    "# Gera relatório rápido com ydata-profiling (salva em HTML)\n",
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title='California Housing - EDA', explorative=True)\n",
    "profile.to_file('california_eda_report.html')\n",
    "print('Relatório salvo: california_eda_report.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99473138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizações customizadas\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df['MedHouseVal'], kde=True)\n",
    "plt.title('Distribuição do alvo (MedHouseVal)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "plt.title('Mapa de correlação')\n",
    "plt.show()\n",
    "\n",
    "# Missing values visualization\n",
    "msno.matrix(df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ee9b5",
   "metadata": {},
   "source": [
    "## 3) Pré-processamento e engenharia de variáveis\n",
    "\n",
    "Criamos colunas opcionais e definimos treino/teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar algumas features engenheiradas (opcionais)\n",
    "df['RoomsPerHousehold'] = df['AveRooms'] / (df['HouseAge']+1)\n",
    "df['BedroomsPerRoom'] = df['AveBedrms'] / (df['AveRooms']+1)\n",
    "df['PopulationPerHousehold'] = df['Population'] / (df['HouseAge']+1)\n",
    "\n",
    "# Alvo\n",
    "y = df['MedHouseVal']\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('Shapes:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4c791",
   "metadata": {},
   "source": [
    "## 4) Lista de modelos (20+)\n",
    "\n",
    "Vamos treinar um conjunto diversificado de modelos para regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model list\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Optional third-party\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42),\n",
    "    'ElasticNet': ElasticNet(random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'SGDRegressor': SGDRegressor(max_iter=1000, tol=1e-3, random_state=42),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'ExtraTrees': ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'HistGradientBoosting': HistGradientBoostingRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'Bagging': BaggingRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(hidden_layer_sizes=(128,64), max_iter=500, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', n_estimators=200, random_state=42, verbosity=0),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=200, random_state=42),\n",
    "    'CatBoost': cb.CatBoostRegressor(verbose=0, random_state=42),\n",
    "}\n",
    "len(models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7109a",
   "metadata": {},
   "source": [
    "## 5) Treinamento básico (pipeline) e avaliação\n",
    "\n",
    "Para cada modelo usaremos um Pipeline com `StandardScaler` quando aplicável. Medidas: RMSE, MAE, R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "from time import time\n",
    "for name, model in models.items():\n",
    "    # pipeline: scaler + model (some tree-based models ignore scaling but it's okay)\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    t0 = time()\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    elapsed = time() - t0\n",
    "    results.append({'model': name, 'rmse': rmse, 'mae': mae, 'r2': r2, 'time_s': elapsed})\n",
    "    print(f'{name}: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}, time={elapsed:.1f}s')\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('rmse')\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02821f4d",
   "metadata": {},
   "source": [
    "## 6) Redes neurais (PyTorch): MLP e 1D-CNN\n",
    "\n",
    "Construiremos uma MLP com PyTorch e uma 1D-CNN aplicando convolução sobre as features (tratadas como sequência)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Prepare scaled data\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test_s = scaler.transform(X_test).astype(np.float32)\n",
    "y_train_s = y_train.to_numpy().astype(np.float32).reshape(-1,1)\n",
    "y_test_s = y_test.to_numpy().astype(np.float32).reshape(-1,1)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_s), torch.from_numpy(y_train_s))\n",
    "test_ds = TensorDataset(torch.from_numpy(X_test_s), torch.from_numpy(y_test_s))\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "# MLP definition\n",
    "class MLPRegressorPyTorch(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader=None, epochs=50, lr=1e-3):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_val = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # optional val\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb = xb.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    preds = model(xb)\n",
    "                    val_loss += criterion(preds, yb).item() * xb.size(0)\n",
    "            val_loss = val_loss / len(val_loader.dataset)\n",
    "            if val_loss < best_val:\n",
    "                best_val = val_loss\n",
    "                best_weights = model.state_dict()\n",
    "    if val_loader is not None:\n",
    "        model.load_state_dict(best_weights)\n",
    "    return model\n",
    "\n",
    "# Instantiate and train MLP\n",
    "mlp_pt = MLPRegressorPyTorch(X_train_s.shape[1])\n",
    "mlp_pt = train_model(mlp_pt, train_loader, val_loader=None, epochs=50)\n",
    "\n",
    "# Predict helper\n",
    "def predict_torch(model, X_np):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xb = torch.from_numpy(X_np).to(device)\n",
    "        preds = model(xb).cpu().numpy().ravel()\n",
    "    return preds\n",
    "\n",
    "preds_mlp_pt = predict_torch(mlp_pt, X_test_s)\n",
    "print('MLP PyTorch RMSE:', mean_squared_error(y_test, preds_mlp_pt, squared=False))\n",
    "\n",
    "# 1D-CNN: reshape features as (batch, channels, seq_len) for Conv1d in PyTorch\n",
    "X_train_c = X_train_s.reshape((X_train_s.shape[0], 1, X_train_s.shape[1]))\n",
    "X_test_c = X_test_s.reshape((X_test_s.shape[0], 1, X_test_s.shape[1]))\n",
    "train_ds_c = TensorDataset(torch.from_numpy(X_train_c), torch.from_numpy(y_train_s))\n",
    "test_ds_c = TensorDataset(torch.from_numpy(X_test_c), torch.from_numpy(y_test_s))\n",
    "train_loader_c = DataLoader(train_ds_c, batch_size=64, shuffle=True)\n",
    "test_loader_c = DataLoader(test_ds_c, batch_size=256, shuffle=False)\n",
    "\n",
    "class Conv1DRegressorPyTorch(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "cnn_pt = Conv1DRegressorPyTorch(X_train_c.shape[2])\n",
    "cnn_pt = train_model(cnn_pt, train_loader_c, val_loader=None, epochs=50)\n",
    "preds_cnn_pt = predict_torch(cnn_pt, X_test_c)\n",
    "print('1D-CNN PyTorch RMSE:', mean_squared_error(y_test, preds_cnn_pt, squared=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f028e5",
   "metadata": {},
   "source": [
    "## 7) AutoML com FLAML\n",
    "\n",
    "Usaremos FLAML para tentar encontrar um bom modelo automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "automl = AutoML()\n",
    "automl_settings = {\n",
    "    'time_budget': 120,  # seconds\n",
    "    'metric': 'rmse',\n",
    "    'task': 'regression',\n",
    "    'log_file_name': 'flaml.log',\n",
    "    'verbose': 0\n",
    "}\n",
    "automl.fit(X_train, y_train, **automl_settings)\n",
    "print('Best model:', automl.best_estimator)\n",
    "preds_automl = automl.predict(X_test)\n",
    "print('AutoML RMSE:', mean_squared_error(y_test, preds_automl, squared=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa63d9",
   "metadata": {},
   "source": [
    "## 8) Consolidação de resultados e comparação\n",
    "\n",
    "Juntamos todas as métricas comparativas em uma tabela ordenada por RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add neural nets and automl to results_df\n",
    "extra = [\n",
    "    {'model':'MLP_PyTorch', 'rmse': mean_squared_error(y_test, preds_mlp_pt, squared=False), 'mae': mean_absolute_error(y_test, preds_mlp_pt), 'r2': r2_score(y_test, preds_mlp_pt), 'time_s': None},\n",
    "    {'model':'CNN1D_PyTorch', 'rmse': mean_squared_error(y_test, preds_cnn_pt, squared=False), 'mae': mean_absolute_error(y_test, preds_cnn_pt), 'r2': r2_score(y_test, preds_cnn_pt), 'time_s': None},\n",
    "    {'model':'FLAML_AutoML', 'rmse': mean_squared_error(y_test, preds_automl, squared=False), 'mae': mean_absolute_error(y_test, preds_automl), 'r2': r2_score(y_test, preds_automl), 'time_s': None},\n",
    "]\n",
    "results_df = pd.concat([results_df, pd.DataFrame(extra)], ignore_index=True).sort_values('rmse')\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ad9f0",
   "metadata": {},
   "source": [
    "## 9) Interpretação: importância de variáveis e partial dependence (exemplo)\n",
    "\n",
    "Mostramos importância do modelo RandomForest e PDP para a variável mais relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (RandomForest)\n",
    "rf = models['RandomForest']\n",
    "pipe_rf = Pipeline([('scaler', StandardScaler()), ('model', rf)])\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "importances = pipe_rf.named_steps['model'].feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
    "display(feat_imp.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410452ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 10\n",
    "plt.figure(figsize=(8,6))\n",
    "feat_imp.head(10).plot(kind='bar')\n",
    "plt.title('Feature importance - RandomForest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial dependence (sklearn)\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "top_feat = feat_imp.index[0]\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "PartialDependenceDisplay.from_estimator(pipe_rf.named_steps['model'], pipe_rf.named_steps['scaler'].transform(X_test), [list(X.columns).index(top_feat)], feature_names=list(X.columns), ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b9d0d",
   "metadata": {},
   "source": [
    "## 10) Conclusões e próximos passos\n",
    "\n",
    "- Resumo dos melhores modelos, sugestões para tuning, possibilidade de usar ensembles, mais AutoML com orçamento maior (AutoGluon/H2O/auto-sklearn), e produção do modelo com MLflow.\n",
    "\n",
    "**Próximos passos sugeridos:**\n",
    "\n",
    "- Aumentar `time_budget` do FLAML e testar AutoGluon/H2O.\n",
    "- Realizar análise de erros por faixa (ex.: bairros, valores altos/baixos).\n",
    "- Otimizar hyperparâmetros com Optuna.\n",
    "- Converter o melhor modelo para produção (ONNX/PyTorch) e versionar com MLflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pycaret)",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
